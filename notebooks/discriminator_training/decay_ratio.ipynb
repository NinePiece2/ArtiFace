{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = os.environ['DATASET_PATH']\n",
    "image_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "NOISE_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MAX_SIZE = 10000\n",
    "\n",
    "# Select percentage of data to use in each epoch\n",
    "RANDOM_SELECT = 0.5\n",
    "\n",
    "INITIAL_TRAIN_RATIO = 5\n",
    "INITIAL_EPOCHS = 20\n",
    "TRAIN_DECAY = 0.8\n",
    "TRAIN_DECAY_EPOCHS = 10\n",
    "\n",
    "\n",
    "INTERVAL = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(128, 128, 3,)),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 64)        4864      \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 128)       204928    \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 131073    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340,865\n",
      "Trainable params: 340,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(32*32*512, use_bias=False, input_shape=(NOISE_DIM,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Reshape((32, 32, 512)),\n",
    "        layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 524288)            52428800  \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 524288)           2097152   \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 524288)            0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 32, 32, 512)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 32, 32, 256)      3276800   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 32, 32, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_9 (Conv2DT  (None, 64, 64, 128)      819200    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64, 64, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_10 (Conv2D  (None, 128, 128, 64)     204800    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 128, 128, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_11 (Conv2D  (None, 128, 128, 3)      4800      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,833,344\n",
      "Trainable params: 57,783,872\n",
      "Non-trainable params: 1,049,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RESUME = False\n",
    "epoch = 0\n",
    "\n",
    "if(TRAINING_RESUME):\n",
    "    generator.load_weights(os.path.join(os.environ['CHECKPOINT_PATH'], f\"generator_decay_based_{epoch}.h5\"))\n",
    "    discriminator.load_weights(os.path.join(os.environ['CHECKPOINT_PATH'], f\"discriminator_decay_based_{epoch}.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator setup\n",
    "discriminator.compile(optimizer=optimizer, loss=loss_object, metrics=['accuracy'])\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator setup\n",
    "noise = layers.Input(shape=(NOISE_DIM,))\n",
    "image = generator(noise)\n",
    "\n",
    "## Combined model\n",
    "validity = discriminator(image)\n",
    "combined = tf.keras.Model(noise, validity)\n",
    "combined.compile(optimizer=optimizer, loss=loss_object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train(epoch):\n",
    "    if epoch < INITIAL_EPOCHS:\n",
    "        ratio = INITIAL_TRAIN_RATIO\n",
    "    \n",
    "    else:\n",
    "        ratio = max(1,INITIAL_TRAIN_RATIO * TRAIN_DECAY ** ((epoch - INITIAL_EPOCHS) // TRAIN_DECAY_EPOCHS))\n",
    "    \n",
    "    return (epoch%ratio == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(epoch):\n",
    "    noise = np.random.normal(0, 1, (1, NOISE_DIM))\n",
    "\n",
    "    gen_imgs = generator.predict(noise, verbose=0)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig = gen_imgs[0]\n",
    "    im = Image.fromarray((fig * 255).astype(np.uint8))\n",
    "\n",
    "    path = os.path.join(os.environ['SAMPLES_PATH'], f\"image_decay_based_{epoch}.jpg\")\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create batches of images from directory\n",
    "\n",
    "def create_batches(images, batch_size=BATCH_SIZE):\n",
    "    while True:\n",
    "        for i in range(0, MAX_SIZE, batch_size):\n",
    "            batch_images = image_files[i:i+batch_size]\n",
    "            batch = np.array([imread(file_name) for file_name in batch_images])\n",
    "            batch = (batch.astype(np.float32) - 127.5) / 127.5\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(generator, discriminator,epoch):\n",
    "    path = os.path.join(os.environ['CHECKPOINT_PATH'], f\"generator_decay_based_{epoch}.h5\")\n",
    "    generator.save(path)\n",
    "\n",
    "    path = os.path.join(os.environ['CHECKPOINT_PATH'], f\"discriminator_decay_based_{epoch}.h5\")\n",
    "    discriminator.save(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = np.ones((BATCH_SIZE, 1))\n",
    "fake = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "discriminator_loss = []\n",
    "discriminator_accuracy = []\n",
    "generator_loss = []\n",
    "\n",
    "image_files = np.array(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeModel = tf.keras.Sequential([layers.Resizing(128,128, input_shape=(218,178,3,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch 1 -----------\n",
      "Discriminator loss: 1.0024115596252159e-05   Accuracy: 1.0\n",
      "Generator loss: 2.546130701830407e-07\n",
      "---------------------------------------\n",
      "----------- Epoch 2 -----------\n",
      "Discriminator loss: 1.8836723029380664e-05   Accuracy: 1.0\n",
      "Generator loss: 6.674786591498449e-13\n",
      "---------------------------------------\n",
      "----------- Epoch 3 -----------\n",
      "Discriminator loss: 6.276465683185961e-06   Accuracy: 1.0\n",
      "Generator loss: 2.448346947403479e-07\n",
      "---------------------------------------\n",
      "----------- Epoch 4 -----------\n",
      "Discriminator loss: 6.065539423616428e-06   Accuracy: 1.0\n",
      "Generator loss: 6.423626786045133e-08\n",
      "---------------------------------------\n",
      "----------- Epoch 5 -----------\n",
      "Discriminator loss: 0.0003548898566805292   Accuracy: 1.0\n",
      "Generator loss: 0.000238133710809052\n",
      "---------------------------------------\n",
      "----------- Epoch 6 -----------\n",
      "Discriminator loss: 0.00044413083105609985   Accuracy: 1.0\n",
      "Generator loss: 0.8949234485626221\n",
      "---------------------------------------\n",
      "----------- Epoch 7 -----------\n",
      "Discriminator loss: 4.88516703831543e-05   Accuracy: 1.0\n",
      "Generator loss: 8.485479354858398\n",
      "---------------------------------------\n",
      "----------- Epoch 8 -----------\n",
      "Discriminator loss: 0.039221612736582756   Accuracy: 0.984375\n",
      "Generator loss: 0.5844826698303223\n",
      "---------------------------------------\n",
      "----------- Epoch 9 -----------\n",
      "Discriminator loss: 0.002408143333013868   Accuracy: 1.0\n",
      "Generator loss: 3.089119491050951e-05\n",
      "---------------------------------------\n",
      "----------- Epoch 10 -----------\n",
      "Discriminator loss: 0.013756226049736142   Accuracy: 0.9921875\n",
      "Generator loss: 0.0003191709110978991\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 11 -----------\n",
      "Discriminator loss: 0.005390770500525832   Accuracy: 1.0\n",
      "Generator loss: 0.05424237251281738\n",
      "---------------------------------------\n",
      "----------- Epoch 12 -----------\n",
      "Discriminator loss: 0.0056463489891029894   Accuracy: 1.0\n",
      "Generator loss: 0.028492920100688934\n",
      "---------------------------------------\n",
      "----------- Epoch 13 -----------\n",
      "Discriminator loss: 0.002748509170487523   Accuracy: 1.0\n",
      "Generator loss: 0.020637838169932365\n",
      "---------------------------------------\n",
      "----------- Epoch 14 -----------\n",
      "Discriminator loss: 0.03927971422672272   Accuracy: 0.9765625\n",
      "Generator loss: 0.06215892732143402\n",
      "---------------------------------------\n",
      "----------- Epoch 15 -----------\n",
      "Discriminator loss: 0.032404554076492786   Accuracy: 0.984375\n",
      "Generator loss: 0.5428396463394165\n",
      "---------------------------------------\n",
      "----------- Epoch 16 -----------\n",
      "Discriminator loss: 0.009197999141179025   Accuracy: 0.9921875\n",
      "Generator loss: 0.13818272948265076\n",
      "---------------------------------------\n",
      "----------- Epoch 17 -----------\n",
      "Discriminator loss: 0.016244448721408844   Accuracy: 1.0\n",
      "Generator loss: 0.4804702401161194\n",
      "---------------------------------------\n",
      "----------- Epoch 18 -----------\n",
      "Discriminator loss: 0.05936172977089882   Accuracy: 0.9921875\n",
      "Generator loss: 0.9970875978469849\n",
      "---------------------------------------\n",
      "----------- Epoch 19 -----------\n",
      "Discriminator loss: 0.06108393892645836   Accuracy: 0.9765625\n",
      "Generator loss: 1.3000085353851318\n",
      "---------------------------------------\n",
      "----------- Epoch 20 -----------\n",
      "Discriminator loss: 0.15766798704862595   Accuracy: 0.9453125\n",
      "Generator loss: 0.44850653409957886\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 21 -----------\n",
      "Discriminator loss: 0.2800989896059036   Accuracy: 0.8828125\n",
      "Generator loss: 4.02241849899292\n",
      "---------------------------------------\n",
      "----------- Epoch 22 -----------\n",
      "Discriminator loss: 0.13931996375322342   Accuracy: 0.953125\n",
      "Generator loss: 0.8056765794754028\n",
      "---------------------------------------\n",
      "----------- Epoch 23 -----------\n",
      "Discriminator loss: 0.03698178939521313   Accuracy: 0.9921875\n",
      "Generator loss: 0.10321670770645142\n",
      "---------------------------------------\n",
      "----------- Epoch 24 -----------\n",
      "Discriminator loss: 0.22342649847269058   Accuracy: 0.9296875\n",
      "Generator loss: 3.1918036937713623\n",
      "---------------------------------------\n",
      "----------- Epoch 25 -----------\n",
      "Discriminator loss: 0.1074360553175211   Accuracy: 0.9609375\n",
      "Generator loss: 0.34385597705841064\n",
      "---------------------------------------\n",
      "----------- Epoch 26 -----------\n",
      "Discriminator loss: 0.051424555480480194   Accuracy: 0.9921875\n",
      "Generator loss: 0.06547954678535461\n",
      "---------------------------------------\n",
      "----------- Epoch 27 -----------\n",
      "Discriminator loss: 0.11710887402296066   Accuracy: 0.9765625\n",
      "Generator loss: 0.4570653736591339\n",
      "---------------------------------------\n",
      "----------- Epoch 28 -----------\n",
      "Discriminator loss: 0.10164138674736023   Accuracy: 0.984375\n",
      "Generator loss: 0.8815634846687317\n",
      "---------------------------------------\n",
      "----------- Epoch 29 -----------\n",
      "Discriminator loss: 0.03495831321924925   Accuracy: 0.9921875\n",
      "Generator loss: 0.006985772401094437\n",
      "---------------------------------------\n",
      "----------- Epoch 30 -----------\n",
      "Discriminator loss: 0.452338270843029   Accuracy: 0.8515625\n",
      "Generator loss: 3.714097499847412\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 31 -----------\n",
      "Discriminator loss: 0.05189253017306328   Accuracy: 0.9921875\n",
      "Generator loss: 1.753804087638855\n",
      "---------------------------------------\n",
      "----------- Epoch 32 -----------\n",
      "Discriminator loss: 0.16204170137643814   Accuracy: 0.9375\n",
      "Generator loss: 0.5467487573623657\n",
      "---------------------------------------\n",
      "----------- Epoch 33 -----------\n",
      "Discriminator loss: 0.07983613759279251   Accuracy: 0.984375\n",
      "Generator loss: 0.016391238197684288\n",
      "---------------------------------------\n",
      "----------- Epoch 34 -----------\n",
      "Discriminator loss: 0.179627925157547   Accuracy: 0.9453125\n",
      "Generator loss: 0.410508930683136\n",
      "---------------------------------------\n",
      "----------- Epoch 35 -----------\n",
      "Discriminator loss: 0.046373751014471054   Accuracy: 0.9921875\n",
      "Generator loss: 0.3180861175060272\n",
      "---------------------------------------\n",
      "----------- Epoch 36 -----------\n",
      "Discriminator loss: 0.05282204411923885   Accuracy: 0.984375\n",
      "Generator loss: 0.05338972806930542\n",
      "---------------------------------------\n",
      "----------- Epoch 37 -----------\n",
      "Discriminator loss: 0.034182554110884666   Accuracy: 0.9921875\n",
      "Generator loss: 0.029332691803574562\n",
      "---------------------------------------\n",
      "----------- Epoch 38 -----------\n",
      "Discriminator loss: 0.10911241173744202   Accuracy: 0.953125\n",
      "Generator loss: 6.000858306884766\n",
      "---------------------------------------\n",
      "----------- Epoch 39 -----------\n",
      "Discriminator loss: 0.043523951433598995   Accuracy: 0.9921875\n",
      "Generator loss: 0.4228982925415039\n",
      "---------------------------------------\n",
      "----------- Epoch 40 -----------\n",
      "Discriminator loss: 0.21059545874595642   Accuracy: 0.9453125\n",
      "Generator loss: 0.8007656931877136\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 41 -----------\n",
      "Discriminator loss: 0.05723871663212776   Accuracy: 0.9765625\n",
      "Generator loss: 0.2984985113143921\n",
      "---------------------------------------\n",
      "----------- Epoch 42 -----------\n",
      "Discriminator loss: 0.05806230567395687   Accuracy: 0.9765625\n",
      "Generator loss: 0.2523437440395355\n",
      "---------------------------------------\n",
      "----------- Epoch 43 -----------\n",
      "Discriminator loss: 0.09122064337134361   Accuracy: 0.96875\n",
      "Generator loss: 1.292183518409729\n",
      "---------------------------------------\n",
      "----------- Epoch 44 -----------\n",
      "Discriminator loss: 0.14205194264650345   Accuracy: 0.9375\n",
      "Generator loss: 1.58089017868042\n",
      "---------------------------------------\n",
      "----------- Epoch 45 -----------\n",
      "Discriminator loss: 0.05646079033613205   Accuracy: 0.984375\n",
      "Generator loss: 0.2171086072921753\n",
      "---------------------------------------\n",
      "----------- Epoch 46 -----------\n",
      "Discriminator loss: 0.030943069607019424   Accuracy: 0.9921875\n",
      "Generator loss: 0.21818363666534424\n",
      "---------------------------------------\n",
      "----------- Epoch 47 -----------\n",
      "Discriminator loss: 0.10178504884243011   Accuracy: 0.953125\n",
      "Generator loss: 1.9535107612609863\n",
      "---------------------------------------\n",
      "----------- Epoch 48 -----------\n",
      "Discriminator loss: 0.11891751736402512   Accuracy: 0.9609375\n",
      "Generator loss: 0.5198329091072083\n",
      "---------------------------------------\n",
      "----------- Epoch 49 -----------\n",
      "Discriminator loss: 0.335896760225296   Accuracy: 0.8515625\n",
      "Generator loss: 1.8643934726715088\n",
      "---------------------------------------\n",
      "----------- Epoch 50 -----------\n",
      "Discriminator loss: 0.005433565005660057   Accuracy: 1.0\n",
      "Generator loss: 0.02905881591141224\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 51 -----------\n",
      "Discriminator loss: 0.06116046570241451   Accuracy: 0.96875\n",
      "Generator loss: 0.383222758769989\n",
      "---------------------------------------\n",
      "----------- Epoch 52 -----------\n",
      "Discriminator loss: 0.15230640769004822   Accuracy: 0.9609375\n",
      "Generator loss: 0.4708583950996399\n",
      "---------------------------------------\n",
      "----------- Epoch 53 -----------\n",
      "Discriminator loss: 0.16326507180929184   Accuracy: 0.9765625\n",
      "Generator loss: 2.822223663330078\n",
      "---------------------------------------\n",
      "----------- Epoch 54 -----------\n",
      "Discriminator loss: 0.04742126911878586   Accuracy: 0.984375\n",
      "Generator loss: 0.17790096998214722\n",
      "---------------------------------------\n",
      "----------- Epoch 55 -----------\n",
      "Discriminator loss: 0.12372210621833801   Accuracy: 0.9765625\n",
      "Generator loss: 1.080280065536499\n",
      "---------------------------------------\n",
      "----------- Epoch 56 -----------\n",
      "Discriminator loss: 0.09439602121710777   Accuracy: 0.9609375\n",
      "Generator loss: 0.14298130571842194\n",
      "---------------------------------------\n",
      "----------- Epoch 57 -----------\n",
      "Discriminator loss: 0.05694957636296749   Accuracy: 0.984375\n",
      "Generator loss: 0.11816736310720444\n",
      "---------------------------------------\n",
      "----------- Epoch 58 -----------\n",
      "Discriminator loss: 0.10101523622870445   Accuracy: 0.9453125\n",
      "Generator loss: 6.7590742111206055\n",
      "---------------------------------------\n",
      "----------- Epoch 59 -----------\n",
      "Discriminator loss: 0.1539749950170517   Accuracy: 0.9609375\n",
      "Generator loss: 2.7376937866210938\n",
      "---------------------------------------\n",
      "----------- Epoch 60 -----------\n",
      "Discriminator loss: 0.053033812902867794   Accuracy: 0.9921875\n",
      "Generator loss: 0.08860798180103302\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 61 -----------\n",
      "Discriminator loss: 0.04531139321625233   Accuracy: 0.9921875\n",
      "Generator loss: 0.07326453924179077\n",
      "---------------------------------------\n",
      "----------- Epoch 62 -----------\n",
      "Discriminator loss: 0.1670394092798233   Accuracy: 0.9375\n",
      "Generator loss: 0.7135558128356934\n",
      "---------------------------------------\n",
      "----------- Epoch 63 -----------\n",
      "Discriminator loss: 0.13372623547911644   Accuracy: 0.9609375\n",
      "Generator loss: 0.29484230279922485\n",
      "---------------------------------------\n",
      "----------- Epoch 64 -----------\n",
      "Discriminator loss: 0.14998562633991241   Accuracy: 0.953125\n",
      "Generator loss: 1.2724318504333496\n",
      "---------------------------------------\n",
      "----------- Epoch 65 -----------\n",
      "Discriminator loss: 0.19771945476531982   Accuracy: 0.9375\n",
      "Generator loss: 1.9495172500610352\n",
      "---------------------------------------\n",
      "----------- Epoch 66 -----------\n",
      "Discriminator loss: 0.29668527841567993   Accuracy: 0.890625\n",
      "Generator loss: 0.41157639026641846\n",
      "---------------------------------------\n",
      "----------- Epoch 67 -----------\n",
      "Discriminator loss: 0.16958092898130417   Accuracy: 0.953125\n",
      "Generator loss: 0.3455768823623657\n",
      "---------------------------------------\n",
      "----------- Epoch 68 -----------\n",
      "Discriminator loss: 0.03691820800304413   Accuracy: 0.9921875\n",
      "Generator loss: 0.21659593284130096\n",
      "---------------------------------------\n",
      "----------- Epoch 69 -----------\n",
      "Discriminator loss: 0.19966448843479156   Accuracy: 0.9375\n",
      "Generator loss: 0.754296600818634\n",
      "---------------------------------------\n",
      "----------- Epoch 70 -----------\n",
      "Discriminator loss: 0.17255821079015732   Accuracy: 0.96875\n",
      "Generator loss: 1.3422858715057373\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 71 -----------\n",
      "Discriminator loss: 0.21249502897262573   Accuracy: 0.9140625\n",
      "Generator loss: 0.619662880897522\n",
      "---------------------------------------\n",
      "----------- Epoch 72 -----------\n",
      "Discriminator loss: 0.2375272735953331   Accuracy: 0.9453125\n",
      "Generator loss: 1.1455254554748535\n",
      "---------------------------------------\n",
      "----------- Epoch 73 -----------\n",
      "Discriminator loss: 0.17833773791790009   Accuracy: 0.9453125\n",
      "Generator loss: 3.2464780807495117\n",
      "---------------------------------------\n",
      "----------- Epoch 74 -----------\n",
      "Discriminator loss: 0.09279323741793633   Accuracy: 0.9765625\n",
      "Generator loss: 0.05564674735069275\n",
      "---------------------------------------\n",
      "----------- Epoch 75 -----------\n",
      "Discriminator loss: 0.21982038766145706   Accuracy: 0.9140625\n",
      "Generator loss: 0.7822941541671753\n",
      "---------------------------------------\n",
      "----------- Epoch 76 -----------\n",
      "Discriminator loss: 0.10823071375489235   Accuracy: 0.9921875\n",
      "Generator loss: 1.20299232006073\n",
      "---------------------------------------\n",
      "----------- Epoch 77 -----------\n",
      "Discriminator loss: 0.02209950890392065   Accuracy: 1.0\n",
      "Generator loss: 0.11783605813980103\n",
      "---------------------------------------\n",
      "----------- Epoch 78 -----------\n",
      "Discriminator loss: 0.06511647626757622   Accuracy: 0.984375\n",
      "Generator loss: 0.12197817862033844\n",
      "---------------------------------------\n",
      "----------- Epoch 79 -----------\n",
      "Discriminator loss: 0.06089412793517113   Accuracy: 0.9765625\n",
      "Generator loss: 0.3632962107658386\n",
      "---------------------------------------\n",
      "----------- Epoch 80 -----------\n",
      "Discriminator loss: 0.041450669057667255   Accuracy: 0.9921875\n",
      "Generator loss: 0.020866945385932922\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 81 -----------\n",
      "Discriminator loss: 0.039986447896808386   Accuracy: 0.9921875\n",
      "Generator loss: 0.08305756002664566\n",
      "---------------------------------------\n",
      "----------- Epoch 82 -----------\n",
      "Discriminator loss: 0.08790422976016998   Accuracy: 0.9765625\n",
      "Generator loss: 1.3498448133468628\n",
      "---------------------------------------\n",
      "----------- Epoch 83 -----------\n",
      "Discriminator loss: 0.02412528544664383   Accuracy: 0.9921875\n",
      "Generator loss: 0.03292805701494217\n",
      "---------------------------------------\n",
      "----------- Epoch 84 -----------\n",
      "Discriminator loss: 0.01495116762816906   Accuracy: 1.0\n",
      "Generator loss: 0.023819366469979286\n",
      "---------------------------------------\n",
      "----------- Epoch 85 -----------\n",
      "Discriminator loss: 0.09734786674380302   Accuracy: 0.9765625\n",
      "Generator loss: 0.018674790859222412\n",
      "---------------------------------------\n",
      "----------- Epoch 86 -----------\n",
      "Discriminator loss: 0.13618656247854233   Accuracy: 0.9765625\n",
      "Generator loss: 0.24059824645519257\n",
      "---------------------------------------\n",
      "----------- Epoch 87 -----------\n",
      "Discriminator loss: 0.05607660859823227   Accuracy: 0.984375\n",
      "Generator loss: 0.041460152715444565\n",
      "---------------------------------------\n",
      "----------- Epoch 88 -----------\n",
      "Discriminator loss: 0.027715107426047325   Accuracy: 0.984375\n",
      "Generator loss: 0.07150158286094666\n",
      "---------------------------------------\n",
      "----------- Epoch 89 -----------\n",
      "Discriminator loss: 0.17868493671994656   Accuracy: 0.9375\n",
      "Generator loss: 2.3449487686157227\n",
      "---------------------------------------\n",
      "----------- Epoch 90 -----------\n",
      "Discriminator loss: 0.03928765282034874   Accuracy: 0.984375\n",
      "Generator loss: 3.0340779630932957e-05\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "----------- Epoch 91 -----------\n",
      "Discriminator loss: 0.2449556440114975   Accuracy: 0.8828125\n",
      "Generator loss: 2.1944785118103027\n",
      "---------------------------------------\n",
      "----------- Epoch 92 -----------\n",
      "Discriminator loss: 0.09225437790155411   Accuracy: 0.984375\n",
      "Generator loss: 0.188517764210701\n",
      "---------------------------------------\n",
      "----------- Epoch 93 -----------\n",
      "Discriminator loss: 0.04779709875583649   Accuracy: 0.984375\n",
      "Generator loss: 0.4334459900856018\n",
      "---------------------------------------\n",
      "----------- Epoch 94 -----------\n",
      "Discriminator loss: 0.048972129821777344   Accuracy: 0.9921875\n",
      "Generator loss: 0.20983251929283142\n",
      "---------------------------------------\n",
      "----------- Epoch 95 -----------\n",
      "Discriminator loss: 0.10448375344276428   Accuracy: 0.984375\n",
      "Generator loss: 0.3568342924118042\n",
      "---------------------------------------\n",
      "----------- Epoch 96 -----------\n",
      "Discriminator loss: 0.07478770986199379   Accuracy: 0.984375\n",
      "Generator loss: 0.6751049757003784\n",
      "---------------------------------------\n",
      "----------- Epoch 97 -----------\n",
      "Discriminator loss: 0.017424222780391574   Accuracy: 0.9921875\n",
      "Generator loss: 0.04191522300243378\n",
      "---------------------------------------\n",
      "----------- Epoch 98 -----------\n",
      "Discriminator loss: 0.0291831037029624   Accuracy: 1.0\n",
      "Generator loss: 0.5549710988998413\n",
      "---------------------------------------\n",
      "----------- Epoch 99 -----------\n",
      "Discriminator loss: 0.1853208914399147   Accuracy: 0.953125\n",
      "Generator loss: 1.3717855215072632\n",
      "---------------------------------------\n",
      "----------- Epoch 100 -----------\n",
      "Discriminator loss: 0.07690997049212456   Accuracy: 0.984375\n",
      "Generator loss: 0.8064019680023193\n",
      "---------------------------------------\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"----------- Epoch {epoch+1} -----------\")\n",
    "\n",
    "    epoch_images = np.random.choice(image_files, int(MAX_SIZE*RANDOM_SELECT), replace=False)\n",
    "\n",
    "    imgGenerator = create_batches(epoch_images)\n",
    "\n",
    "    discriminator_batch_loss = []\n",
    "    discriminator_batch_acc = []\n",
    "    generator_batch_loss = []\n",
    "\n",
    "    for i in range (len(epoch_images)//BATCH_SIZE):\n",
    "\n",
    "        imgs = next(imgGenerator)\n",
    "        imgs = reshapeModel(imgs)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))\n",
    "\n",
    "        discriminator.trainable = discriminator_train(epoch+1)\n",
    "\n",
    "        gen_imgs = generator.predict(noise, verbose=0)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, real)\n",
    "\n",
    "        discriminator_batch_loss.append(d_loss[0])\n",
    "        generator_batch_loss.append(g_loss)\n",
    "        discriminator_batch_acc.append(d_loss[1])\n",
    "\n",
    "    discriminator_loss.append(np.mean(discriminator_batch_loss))\n",
    "    generator_loss.append(np.mean(generator_batch_loss))\n",
    "    discriminator_accuracy.append(np.mean(discriminator_batch_acc))\n",
    "\n",
    "\n",
    "    print(f\"Discriminator loss: {discriminator_loss[epoch]}   Accuracy: {discriminator_accuracy[epoch]}\")\n",
    "    print(f\"Generator loss: {generator_loss[epoch]}\")\n",
    "    print('---------------------------------------')\n",
    "\n",
    "    if((epoch+1) % INTERVAL==0):\n",
    "        sample_image(epoch+1)\n",
    "        create_checkpoint(generator, discriminator, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss and accuracy\n",
    "\n",
    "plt.plot(discriminator_loss, label='Discriminator loss')\n",
    "plt.plot(discriminator_accuracy, label='Discriminator accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c86d1f08c8747f65d29efdac2337fb2fbe4a38445ea9d06a150195f44de4365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
